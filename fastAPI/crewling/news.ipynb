{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'http://qt.some.co.kr/TrendMap/JSON/ServiceHandler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f34c27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… ìë™ ì„¤ì •ëœ ìˆ˜ì§‘ ê¸°ê°„: 20250128 ~ 20250428\n",
      "Total Count: 31502\n",
      "Collect Data Count: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_18840\\3262743490.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['image_url'] = image_urls\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ CSV ì €ì¥ ì™„ë£Œ: C:\\Users\\SMHRD\\Desktop\\ì‹¤ì „\\ESC\\fastAPI\\crewling\\date\\auto_top10_í†µí•©_20250428.csv\n",
      "ğŸš€ ì„œë²„ ì‘ë‹µ: âœ… CSV ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤... (Ctrl+Cë¡œ ì¢…ë£Œ)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 173\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    172\u001b[39m     schedule.run_pending()\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import schedule\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ğŸ”¹ ì €ì¥í•  ë””ë ‰í† ë¦¬\n",
    "SAVE_DIR = r\"C:\\Users\\SMHRD\\Desktop\\ì‹¤ì „\\ESC\\fastAPI\\crewling\\date\"\n",
    "\n",
    "# ğŸ”¹ ë‰´ìŠ¤ ë¶„ë¥˜ í•¨ìˆ˜ (ê³ ìš©/ì‚°ì¬)\n",
    "def assign_label(row):\n",
    "    content = (row['title'] or '') + ' ' + (row['content'] or '')\n",
    "    if any(keyword in content for keyword in ['ì‚°ì¬', 'ì‚°ì—…ì¬í•´', 'ì§ì—…ë³‘', 'ìš”ì–‘', 'ê³µìƒ', 'ì—…ë¬´ìƒì§ˆë³‘', 'ê·¼ê³¨ê²©ê³„']):\n",
    "        return 'ì‚°ì¬'\n",
    "    elif any(keyword in content for keyword in ['ê³ ìš©', 'ë…¸ë™', 'ê·¼ë¡œ', 'ì¼ìë¦¬', 'ì„ê¸ˆ', 'í•´ê³ ', 'ì²´ë¶ˆ', 'í”¼í•´', 'ê³„ì•½', 'ë¶€ë‹¹', 'ì‹ ê³ ']):\n",
    "        return 'ê³ ìš©'\n",
    "    else:\n",
    "        return 'ê¸°íƒ€'\n",
    "\n",
    "# ğŸ”¹ Top5 ë‰´ìŠ¤ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_top5(news_sample):\n",
    "    if news_sample.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(news_sample['content'])\n",
    "    kmeans = KMeans(n_clusters=min(20, len(news_sample)), random_state=42)\n",
    "    news_sample['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    closest_docs = []\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        cluster_indices = np.where(news_sample['cluster'] == i)[0]\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "        cluster_vectors = X[cluster_indices]\n",
    "        distances = np.linalg.norm(cluster_vectors - centroids[i], axis=1)\n",
    "        closest_doc_index = cluster_indices[np.argmin(distances)]\n",
    "        closest_docs.append(closest_doc_index)\n",
    "\n",
    "    issue_top_df = news_sample.iloc[closest_docs]\n",
    "\n",
    "    titles = issue_top_df['title'].fillna('').tolist()\n",
    "    title_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.85, min_df=2)\n",
    "    tfidf_matrix = title_vectorizer.fit_transform(titles)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    similarity_score = cosine_sim.sum(axis=1)\n",
    "    top_indices = similarity_score.argsort()[::-1][:5]\n",
    "    auto_top5_df = issue_top_df.iloc[top_indices]\n",
    "\n",
    "    return auto_top5_df\n",
    "\n",
    "# ğŸ”¹ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def run_job():\n",
    "    today = datetime.today()\n",
    "    three_months_ago = today - timedelta(days=90)\n",
    "    start_date = three_months_ago.strftime('%Y%m%d')\n",
    "    end_date = today.strftime('%Y%m%d')\n",
    "\n",
    "    print(f\"ğŸ“… ìë™ ì„¤ì •ëœ ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}\")\n",
    "\n",
    "    keyword = '((ê³ ìš© || ê·¼ë¡œ || ë…¸ë™ || ì¼ìë¦¬) && (ì„ê¸ˆ || í•´ê³  || ì²´ë¶ˆ || í”¼í•´ || ê³„ì•½ || ë¶€ë‹¹ || ì‹ ê³ )) || (ì‚°ì¬ || ì‚°ì—…ì¬í•´ || ì§ì—…ë³‘ || ìš”ì–‘ || ê³µìƒ || ì—…ë¬´ìƒì§ˆë³‘ || ê·¼ê³¨ê²©ê³„)'\n",
    "    urlString = '  '\n",
    "    doc_list = []\n",
    "    for pageNum in range(1, 2):  # í…ŒìŠ¤íŠ¸ìš© 1í˜ì´ì§€ (ì‹¤ì „ì€ 31ë¡œ ë³€ê²½ ê°€ëŠ¥)\n",
    "        params = {\n",
    "            'keyword': keyword,\n",
    "            'startDate': start_date,\n",
    "            'endDate': end_date,\n",
    "            'source': 'news ',\n",
    "            'lang': 'ko',\n",
    "            'rowPerPage': '500',\n",
    "            'pageNum': pageNum,\n",
    "            'orderType': '1',\n",
    "            'command': 'GetKeywordDocuments'\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(urlString, data=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if 'item' in response.json():\n",
    "                doc_list += response.json()['item']['documentList']\n",
    "            else:\n",
    "                print(f\"âš ï¸ í˜ì´ì§€ {pageNum} ì‘ë‹µì— 'item' ì—†ìŒ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í˜ì´ì§€ {pageNum} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\")\n",
    "            continue\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(\"Total Count:\", response.json()['item']['totalCnt'])\n",
    "    print(\"Collect Data Count:\", len(doc_list))\n",
    "\n",
    "    final_df = pd.DataFrame(doc_list)[['date', 'writerName', 'title', 'content', 'url']]\n",
    " # ğŸ”¹ ì´ë¯¸ì§€ URL ìˆ˜ì§‘\n",
    "    image_urls = []\n",
    "    for url in final_df['url']:\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            res = requests.get(url, headers=headers, timeout=10)\n",
    "            res.raise_for_status()\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "            og_image = soup.find('meta', property='og:image')\n",
    "            if og_image and og_image.get('content'):\n",
    "                image_url = og_image['content']\n",
    "            else:\n",
    "                image_url = None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ URL ì—ëŸ¬: {url} ({e})\")\n",
    "            image_url = None\n",
    "\n",
    "        image_urls.append(image_url)\n",
    "\n",
    "    final_df['image_url'] = image_urls\n",
    "\n",
    "    # ğŸ”¹ ìµœì¢… ì €ì¥\n",
    "    today_str = datetime.today().strftime('%Y%m%d')\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    final_file_path = os.path.join(SAVE_DIR, f\"auto_top10_í†µí•©_{today_str}.csv\")\n",
    "    final_df.to_csv(final_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"ğŸ“ CSV ì €ì¥ ì™„ë£Œ: {final_file_path}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "        \"http://localhost:8087/api/news/save-csv\",\n",
    "        json={\"filePath\": final_file_path}\n",
    "        )\n",
    "        print(f\"ğŸš€ ì„œë²„ ì‘ë‹µ: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Spring ì„œë²„ì— ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    # ğŸ”¹ ì‹¤í–‰ + ìŠ¤ì¼€ì¤„ ì„¤ì •\n",
    "run_job()\n",
    "\n",
    "schedule.every().day.at(\"09:00\").do(run_job)\n",
    "\n",
    "print(\"ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤... (Ctrl+Cë¡œ ì¢…ë£Œ)\")\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966632d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import schedule\n",
    "import requests\n",
    "import pandas as pd\n",
    "import tomotopy as tp\n",
    "from kiwipiepy import Kiwi\n",
    "# from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# [ìˆ˜ì§‘] â†’ [ëª…ì‚¬ì¶”ì¶œ] â†’ [í† í”½ëª¨ë¸ë§] â†’ [êµ°ì§‘í™”] â†’ [ëŒ€í‘œë‰´ìŠ¤/í‚¤ì›Œë“œ ìš”ì•½](3ê°œì›”) -> ì—¬ê¸°ë¶€í„° í•˜ë£¨ì”© ë‹¤ì‹œ ì‹œì‘í•˜ì!!(í•˜ë£¨ ë‰´ìŠ¤ë°ì´í„°)\n",
    "# â†’ dfë¡œ ë‚˜ë¨¸ì§€ ì •ë³´ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸° -> dfì˜ ìœ ì‚¬ë„ ë‹¤ì‹œ ì—…ë°ì´íŠ¸í•˜ê¸°  â†’ [Top10 ì¶”ì¶œ] -> [ì €ì¥/ì „ì†¡]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bfe36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: 20250129 ~ 20250429\n",
      "ì´ ìˆ˜ì§‘ ê±´ìˆ˜: 15000\n",
      "âœ… cluster_summary_20250429.csv ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“ CSV ì €ì¥ ì™„ë£Œ: ./date\\auto_top10_í†µí•©_20250429.csv\n",
      "ğŸš€ ì„œë²„ ì‘ë‹µ: âœ… CSV ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. ì „ì—­ ì„¤ì •\n",
    "# mecab = Mecab()\n",
    "kiwi = Kiwi()\n",
    "gtr_ymd = datetime.today().strftime('%y%m%d')  # ì˜ˆ: 250429\n",
    "SAVE_DIR = './date'\n",
    "\n",
    "def extract_nouns(text):\n",
    "    result = kiwi.analyze(text)\n",
    "    nouns = []\n",
    "    for token, pos, _, _ in result[0][0]:\n",
    "        if pos.startswith('N') and len(token) > 1:\n",
    "            nouns.append(token)\n",
    "    return nouns\n",
    "\n",
    "# 2. ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜\n",
    "def collect_news():\n",
    "    today = datetime.today()\n",
    "    three_months_ago = today - timedelta(days=90)\n",
    "    start_date = three_months_ago.strftime('%Y%m%d')\n",
    "    end_date = today.strftime('%Y%m%d')\n",
    "\n",
    "    print(f\"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}\")\n",
    "\n",
    "    keyword = '((ê³ ìš© || ê·¼ë¡œ || ë…¸ë™ || ì¼ìë¦¬) && (ì„ê¸ˆ || í•´ê³  || ì²´ë¶ˆ || í”¼í•´ || ê³„ì•½ || ë¶€ë‹¹ || ì‹ ê³ )) || (ì‚°ì¬ || ì‚°ì—…ì¬í•´ || ì§ì—…ë³‘ || ìš”ì–‘ || ê³µìƒ || ì—…ë¬´ìƒì§ˆë³‘ || ê·¼ê³¨ê²©ê³„)'\n",
    "    urlString = 'http://qt.some.co.kr/TrendMap/JSON/ServiceHandler'\n",
    "\n",
    "    doc_list = []\n",
    "    for pageNum in range(1, 31):  # í…ŒìŠ¤íŠ¸ëŠ” 1, ì‹¤ì „ì€ 31\n",
    "        params = {\n",
    "            'keyword': keyword,\n",
    "            'startDate': start_date,\n",
    "            'endDate': end_date,\n",
    "            'source': 'news ',\n",
    "            'lang': 'ko',\n",
    "            'rowPerPage': '500',\n",
    "            'pageNum': pageNum,\n",
    "            'orderType': '1',\n",
    "            'command': 'GetKeywordDocuments'\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(urlString, data=params)\n",
    "            response.raise_for_status()\n",
    "            if 'item' in response.json():\n",
    "                doc_list += response.json()['item']['documentList']\n",
    "            else:\n",
    "                print(f\"âš ï¸ í˜ì´ì§€ {pageNum} ì‘ë‹µì— 'item' ì—†ìŒ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í˜ì´ì§€ {pageNum} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\")\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(\"ì´ ìˆ˜ì§‘ ê±´ìˆ˜:\", len(doc_list))\n",
    "\n",
    "    df = pd.DataFrame(doc_list)[['date', 'writerName', 'title', 'content', 'url', 'vks']]\n",
    "    df.fillna('', inplace=True)\n",
    "    df['sentence'] = df[['title', 'content']].apply(\" \".join, axis=1)\n",
    "    df['nouns'] = df['sentence'].apply(lambda x: ', '.join(extract_nouns(x)))\n",
    "\n",
    "    return df\n",
    "# 3. í† í”½ ëª¨ë¸ë§ í•¨ìˆ˜\n",
    "def extract_lda_topic(df):\n",
    "    _df = df.copy() \n",
    "    corpus = df['nouns'].str.split(', ').tolist()\n",
    "\n",
    "    # í† í”½ ê°œìˆ˜ ì„¤ì •\n",
    "    if len(corpus) >= 1000: k = 100\n",
    "    elif len(corpus) >= 500: k = 70\n",
    "    elif len(corpus) >= 200: k = 40\n",
    "    else: k = 20\n",
    "\n",
    "    \n",
    "    lda_model = tp.LDAModel(tw=tp.TermWeight.PMI, min_df=3, rm_top=0, k=k, seed=572)\n",
    "\n",
    "    for lis in corpus:\n",
    "        if lis != []:\n",
    "            lda_model.add_doc(lis)\n",
    "\n",
    "    for i in range(0, 200, 4):\n",
    "        lda_model.train(4, workers=1)\n",
    "\n",
    "    word_df = pd.DataFrame()\n",
    "    top_n = 10\n",
    "    for i in range(lda_model.k):\n",
    "        topic_words = [keyword for keyword, score in lda_model.get_topic_words(i, top_n=top_n)]\n",
    "        topic_scores = [score for keyword, score in lda_model.get_topic_words(i, top_n=top_n)]\n",
    "        topic_seq = list(range(len(topic_words)))\n",
    "        _word_df = pd.DataFrame(data={'word_seq': topic_seq, 'word': topic_words, 'pmi_score': topic_scores})\n",
    "        _word_df['topic_id'] = f'topic_{gtr_ymd}_{i}'\n",
    "        # _word_df['embassy_cd'] = _df['embassy_cd'].unique()[0]\n",
    "        _word_df = _word_df[['topic_id', 'word_seq', 'word', 'pmi_score']]\n",
    "        word_df = pd.concat([word_df, _word_df])\n",
    "\n",
    "    word_df = word_df.reset_index(drop=True)\n",
    "    word_df = word_df[['topic_id','word_seq','word','pmi_score']]\n",
    "    word_df['pmi_score'] = word_df.pmi_score.round(5)\n",
    "\n",
    "    topic_dist = [lda_model.infer((lda_model.docs[i]))[0] for i in range(len(_df))]\n",
    "\n",
    "    _df['max_topic_num'] = [vec.argmax() for vec in topic_dist]\n",
    "    _df['max_topic_value'] = [vec.max() for vec in topic_dist] \n",
    "\n",
    "    use_index = _df['max_topic_num'].value_counts().index.tolist()\n",
    "\n",
    "    topic_items = []\n",
    "    for i in use_index:\n",
    "        topic_dict = dict()\n",
    "        topic_words = [keyword for keyword, score in lda_model.get_topic_words(i, top_n=20) if score >= 0.01]\n",
    "        if len(topic_words) >= 5:\n",
    "            topic_dict['mdl_index'] = i\n",
    "            topic_dict['topic_words'] = ', '.join(topic_words[:5])\n",
    "            topic_items.append(topic_dict)\n",
    "\n",
    "    topic_df = pd.DataFrame(topic_items)\n",
    "    if len(topic_df) == 0:\n",
    "        return word_df, topic_df\n",
    "\n",
    "    union_dict = dict()\n",
    "    mdl_index_list = topic_df['mdl_index'].tolist()\n",
    "    mdl_index_list.reverse()\n",
    "    count = 0\n",
    "    for i in mdl_index_list[1:]:\n",
    "        count += 1\n",
    "        target_topic_words = topic_df['topic_words'][topic_df['mdl_index'] == i].tolist()[0].split(', ')\n",
    "\n",
    "        for j in topic_df['mdl_index'].tolist()[-count:]:\n",
    "            subject_topic_words = topic_df['topic_words'][topic_df['mdl_index'] == j].tolist()[0].split(', ')\n",
    "            union = set(target_topic_words) & set(subject_topic_words)\n",
    "            if len(union) >= 3:\n",
    "                union_dict[j] = i\n",
    "\n",
    "    _df['new_topic_num'] = _df['max_topic_num']\n",
    "    for old, new in union_dict.items():\n",
    "        _df.loc[_df['max_topic_num'] == old, 'new_topic_num'] = new\n",
    "\n",
    "    info_df = pd.DataFrame(data={'topic_num':range(lda_model.k)})\n",
    "\n",
    "    info_df['new_topic_num'] = info_df['topic_num']\n",
    "    for old, new in union_dict.items():\n",
    "        info_df.loc[info_df['topic_num'] == old, 'new_topic_num'] = new\n",
    "\n",
    "    count_df = _df['max_topic_num'].value_counts().reset_index()\n",
    "    count_df.columns = ['topic_num', 'doc_cnt']\n",
    "    info_df = info_df.merge(count_df, how='left').fillna(0)\n",
    "    info_df.doc_cnt = info_df.doc_cnt.astype('int')\n",
    "\n",
    "    _df = _df[_df['max_topic_value'] >= 0.5].reset_index(drop=True)\n",
    "\n",
    "    temp_df = _df['max_topic_num'].value_counts()\n",
    "    use_index = temp_df[temp_df >= 3].index.tolist()\n",
    "\n",
    "    _df = _df[_df['max_topic_num'].isin(use_index)].reset_index(drop=True)\n",
    "    _df = _df[_df['max_topic_num'].isin(topic_df['mdl_index'].tolist())].reset_index(drop=True)\n",
    "\n",
    "    topic_df = topic_df[topic_df['mdl_index'].isin(_df['new_topic_num'].unique().tolist())]\n",
    "\n",
    "    _df.reset_index(inplace=True)\n",
    "    _df.rename(columns={'index':'docid'}, inplace=True)\n",
    "\n",
    "    top_df = pd.DataFrame()\n",
    "    new_topic_number = 0\n",
    "    for topic_number in _df['new_topic_num'].value_counts().index:\n",
    "        temp_df = _df[_df['new_topic_num'] == topic_number].reset_index(drop=True)\n",
    "\n",
    "        corpus = temp_df['nouns'].tolist()\n",
    "        docid = temp_df['docid'].tolist()\n",
    "\n",
    "        # tf-idf ì ìš©\n",
    "        tfidfv = TfidfVectorizer().fit(corpus)\n",
    "        vector = tfidfv.transform(corpus).toarray()\n",
    "\n",
    "        # kê°’ ì„¤ì •, í•™ìŠµ\n",
    "        km = KMeans(n_clusters = 1)\n",
    "        km.fit(vector)\n",
    "\n",
    "        # í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¡œ ë°ì´í„°í”„ë ˆì„ ì¬êµ¬ì¶•\n",
    "        results = []\n",
    "        clusters = km.labels_.tolist()  # êµ°ì§‘í™” ê²°ê³¼(ë¼ë²¨)\n",
    "        for i, value in enumerate(clusters):\n",
    "            \n",
    "            result_dict = {}\n",
    "            result_dict['mdl_index'] = topic_number\n",
    "            result_dict['title'] = temp_df.loc[temp_df.docid == docid[i]]['title'].tolist()[0]\n",
    "            result_dict['nouns'] = temp_df.loc[temp_df.docid == docid[i]]['nouns'].tolist()[0]\n",
    "            result_dict['topic_value'] = temp_df.loc[temp_df.docid == docid[i]]['max_topic_value'].tolist()[0]\n",
    "            vec1 = vector[i].reshape(1, -1)  # ë‰´ìŠ¤ ë²¡í„°\n",
    "            vec2 = km.cluster_centers_[int(value)].reshape(1,-1)  # kì¤‘ì•™ê°’ ë²¡í„°\n",
    "            result_dict['similarity'] = cosine_similarity(vec1, vec2)[0][0]  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "            results.append(result_dict)\n",
    "        \n",
    "        result_df = pd.DataFrame(results).sort_values(['mdl_index','similarity'], ascending=[True, False]).reset_index(drop=True)\n",
    "        top_df = pd.concat([top_df, result_df])\n",
    "\n",
    "    top_df = top_df[top_df['similarity'] >= 0.2].reset_index(drop=True)\n",
    "\n",
    "    topic_df['doc_count'] = 0\n",
    "    for mdl_index, doc_count in top_df['mdl_index'].value_counts().items():\n",
    "        topic_df.loc[topic_df['mdl_index'] == mdl_index, 'doc_count'] = doc_count\n",
    "\n",
    "    return top_df, topic_df, info_df, word_df\n",
    "\n",
    "# 4. ì´ìŠˆ í‚¤ì›Œë“œ ì¶”ì¶œ \n",
    "def extract_issue_kwd(top_df, topic_df, info_df):\n",
    "    stopwords = [\n",
    "    'í†µë³´', 'ê³„íš', 'ì†¡ë¶€', 'ë³´ê³ ', 'íšŒì‹ ', 'ê²°ê³¼', 'ì œì¶œ', 'ì°¸ì„', 'ê³µì§€', 'ì•ˆë‚´', 'ìš”ì²­', 'ì ‘ìˆ˜', 'í™•ì¸', \n",
    "    'ì¡°ì¹˜', 'ì§„í–‰', 'ê´€ë¦¬', 'ê³µê³ ', 'ë³€ê²½', 'ê°œì •', 'ì‹ ì²­', 'ì ‘ìˆ˜', 'í™•ëŒ€', 'ì¶•ì†Œ', 'ê°œì„ ', 'ì´í–‰',\n",
    "    'íšŒì˜', 'ë³´ê³ ì„œ', 'ë¬¸ì„œ', 'ì„œë¥˜', 'ìë£Œ', 'íŒŒì¼', 'ì‘ì„±', 'ë°°í¬', 'í™•ì‚°', 'ì‚¬ì—…', 'ì „ë‹¬',\n",
    "    'ì›”', 'ì£¼ë…„', 'ë¶„ê¸°', 'ê¸°ê°„', 'ì‹œí–‰', 'ì˜¤ì „', 'ì˜¤í›„', 'ì¡°ì‚¬', 'í‰ê°€', 'ì§„ë‹¨', 'ëŒ€ì±…', 'ë°©ì•ˆ',\n",
    "    'ì œë„', 'ì •ì±…', 'ì§€ì›', 'í”„ë¡œê·¸ë¨', 'ì°¸ì—¬'\n",
    "]\n",
    "    main_keywords = []\n",
    "    for mdl_index in top_df['mdl_index'].value_counts().index:\n",
    "        main_keyword_dict = dict()\n",
    "\n",
    "        # í† í”½ë‹¨ì–´ ì¤‘ ë³´í¸ì ì¸ ë‹¨ì–´(stopwords) ì œê±°\n",
    "        try:\n",
    "            re_ngram = ', '.join(topic_df['topic_words'][topic_df['mdl_index'] == mdl_index]).split(', ')\n",
    "            re_ngram = [x for x in re_ngram if x not in stopwords]\n",
    "        except:\n",
    "            break\n",
    "        # ì œëª©ì—ì„œ ()ê´„í˜¸ ì‚­ì œ\n",
    "        morph_title = top_df[top_df['mdl_index'] == mdl_index].title.str.replace(r'\\(\\w.*\\)', '', regex=True).tolist()\n",
    "\n",
    "        keyword_list = []\n",
    "        for title in morph_title:\n",
    "            try:\n",
    "                # ì œëª©ì—ì„œ ëª…ì‚¬ë§Œ ë‚¨ê¹€(title)\n",
    "                query = ' '.join(extract_nouns(title))\n",
    "\n",
    "                # í† í”½ë‹¨ì–´ top1 1ê°œë¡œ ì¿¼ë¦¬ì—ì„œ 6ì ì´ìƒ ë‹¨ì–´ ì¡°íšŒ\n",
    "                use_word = r'\\w*{}\\w*'.format(re_ngram[0])\n",
    "                keywords = re.findall(use_word, query)\n",
    "                if keywords != []:\n",
    "                    keyword = keywords[0]\n",
    "                    if len(keyword) > 5:\n",
    "                        keyword_list.append(keyword)\n",
    "\n",
    "                # í† í”½ë‹¨ì–´ top1~2 2ê°œë¡œ ì¿¼ë¦¬ì—ì„œ ë‹¨ì–´ ì¡°íšŒ\n",
    "                use_word = r'\\w*{}\\w*|\\w*{}\\w*'.format(re_ngram[0], re_ngram[1])\n",
    "                keywords = re.findall(use_word, query)\n",
    "                if len(keywords) == 2:\n",
    "                    keyword = ' '.join(keywords)\n",
    "                    keyword_list.append(keyword)\n",
    "\n",
    "                # í† í”½ë‹¨ì–´ top2 1ê°œë¡œ ì¿¼ë¦¬ì—ì„œ 6ì ì´ìƒ ë‹¨ì–´ ì¡°íšŒ\n",
    "                use_word = r'\\w*{}\\w*'.format(re_ngram[1])\n",
    "                keywords = re.findall(use_word, query)\n",
    "                if keywords != []:\n",
    "                    keyword = keywords[0]\n",
    "                    if len(keyword) > 5:\n",
    "                        keyword_list.append(keyword)\n",
    "                        continue\n",
    "\n",
    "                # í† í”½ë‹¨ì–´ top1~3, 3ê°œë¡œ ì¿¼ë¦¬ì—ì„œ ë‹¨ì–´ ì¡°íšŒ\n",
    "                use_word = r'\\w*{}\\w*|\\w*{}\\w*|\\w*{}\\w*'.format(re_ngram[0], re_ngram[1], re_ngram[2])\n",
    "                keywords = re.findall(use_word, query)\n",
    "                if len(keywords) == 2:\n",
    "                    keyword = ' '.join(keywords)\n",
    "                    keyword_list.append(keyword)\n",
    "\n",
    "                # í† í”½ë‹¨ì–´ top1~3 2ê°œë¡œ ì œëª©ì—ì„œ ë‹¨ì–´ ì¡°íšŒ\n",
    "                use_word = r'\\w*{}\\w*|\\w*{}\\w*|\\w*{}\\w*'.format(re_ngram[0], re_ngram[1], re_ngram[2])\n",
    "                keywords = re.findall(use_word, query)\n",
    "                if len(keywords) == 2:\n",
    "                    keyword = ' '.join(keywords)\n",
    "                    keyword_list.append(keyword)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        try:\n",
    "            main_kwd = max([kwd for kwd, cnt in Counter(keyword_list).items() if (cnt == max(Counter(keyword_list).values())) & (cnt >= 2)], key=len)\n",
    "            main_keyword_dict['mdl_index'] = mdl_index\n",
    "            main_keyword_dict['main_kwd'] = main_kwd\n",
    "            main_keywords.append(main_keyword_dict)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    issue_df = pd.DataFrame(main_keywords)\n",
    "    issue_df.columns = ['new_topic_num', 'iss_kwd']\n",
    "    issue_df = issue_df.merge(info_df.groupby(['new_topic_num'])['doc_cnt'].sum().reset_index(), how='left')\n",
    "    issue_df = issue_df.sort_values(['doc_cnt', 'iss_kwd'], ascending=[False, True]).reset_index(drop=True)\n",
    "    issue_df['topic_id'] = f'topic_{gtr_ymd}_' + issue_df.new_topic_num.astype('string')\n",
    "    issue_df['use_yn'] = 'Y'\n",
    "    issue_df = issue_df[~issue_df.iss_kwd.duplicated()].reset_index(drop=True)\n",
    "    issue_df['kwd_rank'] = issue_df.index + 1\n",
    "    issue_df = issue_df[['topic_id', 'kwd_rank', 'iss_kwd', 'doc_cnt', 'use_yn']]\n",
    "    issue_df.rename(columns={'iss_kwd':'topic_iss_kwd'}, inplace=True)\n",
    "\n",
    "    return issue_df\n",
    "\n",
    "# 5. ëŒ€í‘œ ë‰´ìŠ¤ ë° ìš”ì•½ \n",
    "def summarize_clusters(top_df):\n",
    "    cluster_summary = []\n",
    "\n",
    "    for mdl_index in sorted(top_df['mdl_index'].unique()):\n",
    "        cluster_info = {}\n",
    "\n",
    "        # í•´ë‹¹ êµ°ì§‘ ë°ì´í„°\n",
    "        cluster_df = top_df[top_df['mdl_index'] == mdl_index]\n",
    "\n",
    "        # 1. ëŒ€í‘œ ë‰´ìŠ¤ (similarity ê°€ì¥ ë†’ì€ ì œëª©)\n",
    "        best_news = cluster_df.sort_values(by='similarity', ascending=False).iloc[0]\n",
    "        cluster_info['mdl_index'] = mdl_index\n",
    "        cluster_info['ëŒ€í‘œë‰´ìŠ¤ì œëª©'] = best_news['title']\n",
    "\n",
    "        # 2. êµ°ì§‘ ë‚´ ëª…ì‚¬ ëª¨ì•„ì„œ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œ ì¶”ì¶œ\n",
    "        all_nouns = []\n",
    "        for nouns in cluster_df['nouns']:\n",
    "            all_nouns += nouns.split(', ')\n",
    "        noun_counts = Counter(all_nouns)\n",
    "        top_nouns = [noun for noun, count in noun_counts.most_common(5)]\n",
    "        cluster_info['ëŒ€í‘œí‚¤ì›Œë“œ'] = ', '.join(top_nouns)\n",
    "\n",
    "        # 3. êµ°ì§‘ í‰ê·  similarity\n",
    "        avg_similarity = cluster_df['similarity'].mean()\n",
    "        cluster_info['êµ°ì§‘í‰ê· ìœ ì‚¬ë„'] = round(avg_similarity, 4)\n",
    "\n",
    "        # 4. ë‰´ìŠ¤ ê°œìˆ˜ (doc ìˆ˜)\n",
    "        cluster_info['ë‰´ìŠ¤ê°œìˆ˜'] = len(cluster_df)\n",
    "\n",
    "        cluster_summary.append(cluster_info)\n",
    "\n",
    "    # ì „ì²´ êµ°ì§‘ ìš”ì•½ ë°ì´í„°í”„ë ˆì„\n",
    "    issue_summary_df = pd.DataFrame(cluster_summary)\n",
    "\n",
    "    # ğŸ”¥ ì—¬ê¸°ì„œ Top10ë§Œ ì¶”ì¶œ\n",
    "    issue_summary_df = issue_summary_df.sort_values(\n",
    "        by=['ë‰´ìŠ¤ê°œìˆ˜', 'êµ°ì§‘í‰ê· ìœ ì‚¬ë„'], \n",
    "        ascending=[False, False]\n",
    "    ).head(10).reset_index(drop=True)\n",
    "\n",
    "    return issue_summary_df\n",
    "\n",
    "# 6. ì €ì¥ ë° ì „ì†¡\n",
    "def save_and_send(top_df):\n",
    "    today_str = datetime.today().strftime('%Y%m%d')\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    final_file_path = os.path.join(SAVE_DIR, f\"auto_top10_í†µí•©_{today_str}.csv\")\n",
    "    top_df.to_csv(final_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"ğŸ“ CSV ì €ì¥ ì™„ë£Œ: {final_file_path}\")\n",
    "\n",
    "    # try:\n",
    "    #     response = requests.post(\n",
    "    #         \"http://localhost:8087/api/news/save-csv\",\n",
    "    #         json={\"filePath\": final_file_path}\n",
    "    #     )\n",
    "    #     print(f\"ğŸš€ ì„œë²„ ì‘ë‹µ: {response.text}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"âŒ ì„œë²„ ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# 7. ë©”ì¸ ì‹¤í–‰ \n",
    "def main():\n",
    "    df = collect_news()\n",
    "    _df = df.copy()\n",
    "    top_df, topic_df, info_df, word_df = extract_lda_topic(_df)\n",
    "    issue_summary_df = summarize_clusters(top_df)\n",
    "\n",
    "    today_str = datetime.today().strftime('%Y%m%d')\n",
    "    issue_summary_df.to_csv(f'./date/cluster_summary_{today_str}.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… cluster_summary_{today_str}.csv ì €ì¥ ì™„ë£Œ\")\n",
    "    save_and_send(top_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# # 8. ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "# run_job()  # ì²˜ìŒ 1ë²ˆ ì‹¤í–‰\n",
    "\n",
    "# schedule.every().day.at(\"09:00\").do(run_job)\n",
    "\n",
    "# print(\"ìŠ¤ì¼€ì¤„ëŸ¬ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤... (Ctrl+Cë¡œ ì¢…ë£Œ)\")\n",
    "\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(1)   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
