import os
import time
import requests
import pandas as pd
from datetime import datetime
from bs4 import BeautifulSoup
from sqlalchemy import create_engine
from dotenv import load_dotenv
import random

urlString = 'http://qt.some.co.kr/TrendMap/JSON/ServiceHandler'
keyword = '(ê³ ìš©ë³´í—˜||ì„ê¸ˆì²´ë¶ˆ||ì‚°ì¬ë³´í—˜||ë¶€ë‹¹í•´ê³ ||ì‹¤ì—…ê¸‰ì—¬||ê·¼ë¡œê³„ì•½||ì‚°ì—…ì¬í•´||ì—…ë¬´ìƒì§ˆë³‘)&&~(#@VK#S1#ìŠ¤í¬ì¸ )&&~(#@VK#S1#TVì—°ì˜ˆ)'

def run_job():
    today = datetime.today()
    date_str = today.strftime('%Y%m%d')
    print(f"ğŸ“† ìë™ ìˆ˜ì§‘ ê¸°ê°„: {date_str}")

    doc_list = []

    for pageNum in range(1, 2):
        params = {
            'keyword': keyword,
            'startDate': date_str,
            'endDate': date_str,
            'source': 'news',
            'lang': 'ko',
            'rowPerPage': '10',
            'pageNum': pageNum,
            'orderType': '1',
            'command': 'GetKeywordDocuments'
        }

        try:
            response = requests.post(urlString, data=params, timeout=10)
            response.raise_for_status()
            json_data = response.json()
            documents = json_data.get('item', {}).get('documentList', [])
            doc_list += documents
        except Exception as e:
            print(f"âŒ ìš”ì²­ or ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return

        time.sleep(1)

    print(f"ğŸ“„ ìˆ˜ì§‘ëœ ë¬¸ì„œ ìˆ˜: {len(doc_list)}")
    if not doc_list:
        print("âš ï¸ ë¬¸ì„œ ì—†ìŒ. ì¢…ë£Œ.")
        return

    df = pd.DataFrame(doc_list)

    if 'writer' in df.columns:
        writer_col = 'writer'
    elif 'writerName' in df.columns:
        writer_col = 'writerName'
    else:
        df['writer'] = 'ì •ë³´ ì—†ìŒ'
        writer_col = 'writer'

    final_df = df[['title', writer_col, 'url', 'date']].copy()
    final_df.columns = ['news_title', 'news_writer', 'news_url', 'date']

    headers_list = [
        {'User-Agent': 'Mozilla/5.0'},
        {'User-Agent': 'Chrome/120.0.0.0'},
        {'User-Agent': 'Safari/537.36'},
    ]
    image_urls = []
    for i, url in enumerate(final_df['news_url']):
        try:
            headers = random.choice(headers_list)
            res = requests.get(url, headers=headers, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, 'html.parser')
            og_image = soup.find('meta', property='og:image')
            image_url = og_image['content'] if og_image and og_image.get('content') else None
        except Exception as e:
            if i < 3:  
                print(f"âŒ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {url} ({e})")
            image_url = None
        image_urls.append(image_url)

    final_df['news_img'] = image_urls
    final_df['crawled_at'] = datetime.now()
    final_df = final_df[['news_title', 'news_writer', 'news_img', 'news_url', 'date', 'crawled_at']]

    load_dotenv('.env')
    DB_URL = os.getenv("DB_URL")

    try:
        engine = create_engine(DB_URL)
    except Exception as e:
        print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
        exit()

    try:
        final_df.to_sql(name='tbl_news', con=engine, if_exists='append', index=False)
        print(f"âœ… DB ì €ì¥ ì™„ë£Œ - {len(final_df)}ê±´")
    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    run_job()
